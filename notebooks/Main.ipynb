{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "- [Environment Setup](#Environment_Setup)\n",
    "- [Ensure Reproducibility](#Ensure_Reproducibility)\n",
    "- [HyperParameters - All](#HyperParameters_-_All)\n",
    "- [Competition Supplied Data](#Competition_Supplied_Data)\n",
    "- [Import Data](#Import_Data)\n",
    "- [PreProcessing](#PreProcessing)\n",
    "- [Initial Filtering](#Initial_Filtering)\n",
    "- [Feature Engineering](#Feature_Engineering)\n",
    "- [Feature Reduction](#Feature_Reduction)\n",
    "- [CV Folds](#CV_Folds)\n",
    "- [PyTorch Setup](#PyTorch_Setup)\n",
    "- [Training Process](#Training_Process)\n",
    "- [Results](#Results)\n",
    "- [Prepare Submission File](#Prepare_Submission_File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Environment_Setup'></a>\n",
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:19.629372Z",
     "start_time": "2020-11-18T19:14:18.050137Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:21.636016Z",
     "start_time": "2020-11-18T19:14:21.626946Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HyperParameters_-_All'></a>\n",
    "# HyperParameters - All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:24.767741Z",
     "start_time": "2020-11-18T19:14:24.763785Z"
    }
   },
   "outputs": [],
   "source": [
    "# INITIAL FILTERING PARAMETERS\n",
    "Include_neg_10 = False\n",
    "Variance_Threshold = True\n",
    "VAR_THRESH=0.5\n",
    "PCA_VAR = True\n",
    "PCA_VAR_THRESH = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:25.566700Z",
     "start_time": "2020-11-18T19:14:25.562711Z"
    }
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING PARAMETERS\n",
    "Category_Encoding = \"OHE\"  # \"OHE\" or \"Mapping\"\n",
    "Normalization = True\n",
    "Scaling = False\n",
    "Remove_Skewness = False\n",
    "Quantile_Transforming = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:26.312011Z",
     "start_time": "2020-11-18T19:14:26.308960Z"
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:26.884921Z",
     "start_time": "2020-11-18T19:14:26.881071Z"
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE REDUCTION PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:28.321388Z",
     "start_time": "2020-11-18T19:14:27.398358Z"
    }
   },
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7\n",
    "HIDDEN_SIZE=1024\n",
    "\n",
    "Label_Smoothing = True\n",
    "SMOOTHING = 0.000625\n",
    "CLAMPING = False\n",
    "CLAMP_MIN = 0.0001\n",
    "CLAMP_MAX = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:28.439864Z",
     "start_time": "2020-11-18T19:14:28.435768Z"
    }
   },
   "outputs": [],
   "source": [
    "# POSTPROCESSING PARAMETERS\n",
    "FORCE_BAD_COLS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded Features to Drop or Keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:29.970638Z",
     "start_time": "2020-11-18T19:14:29.966550Z"
    }
   },
   "outputs": [],
   "source": [
    "KEPT_FEATURES = ['g-307']\n",
    "DROPPED_FEATURES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Competition_Supplied_Data'></a>\n",
    "# Competition Supplied Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:36.114401Z",
     "start_time": "2020-11-18T19:14:31.997529Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 training related files\n",
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "\n",
    "# 2 test related files\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat/Reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:39.979872Z",
     "start_time": "2020-11-18T19:14:39.779458Z"
    }
   },
   "outputs": [],
   "source": [
    "# train merges the train features and targets\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:41.088907Z",
     "start_time": "2020-11-18T19:14:40.623666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then train drops the control cases and resets the pandas index\n",
    "ctrl_train = train[train_features['cp_type'] == 'ctl_vehicle'].reset_index(drop=True)\n",
    "train = train[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "# Similarly drop the control cases from the test set. Will later fill with 0.\n",
    "ctrl_test = test_features[test_features['cp_type'] == 'ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "# Also drop these control cases from the nonscored target df\n",
    "targets_nonscored = train_targets_nonscored[train_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:41.762987Z",
     "start_time": "2020-11-18T19:14:41.739981Z"
    }
   },
   "outputs": [],
   "source": [
    "# target is a subset of train with just sig_id and the 206 target columns\n",
    "target = train[train_targets_scored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:42.621736Z",
     "start_time": "2020-11-18T19:14:42.464936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the cp_type column. Homogenous in each df anyways\n",
    "train = train.drop('cp_type', axis=1)\n",
    "ctrl_train = ctrl_train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "ctrl_test = ctrl_test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:43.792833Z",
     "start_time": "2020-11-18T19:14:43.784710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (21948, 1081)\n",
      "ctrl_train: (1866, 1081)\n",
      "target: (21948, 207)\n",
      "nonscored_target: (21948, 403)\n",
      "\n",
      "test: (3624, 875)\n",
      "ctrl_test: (358, 875)\n",
      "sample_submission: (3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# Data Dimensions\n",
    "print(f\"train: {train.shape}\")\n",
    "print(f\"ctrl_train: {ctrl_train.shape}\")\n",
    "print(f\"target: {target.shape}\")\n",
    "print(f\"nonscored_target: {targets_nonscored.shape}\")\n",
    "print()\n",
    "print(f\"test: {test.shape}\")\n",
    "print(f\"ctrl_test: {ctrl_test.shape}\")\n",
    "print(f\"sample_submission: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Feature columns\n",
    "Forcing all feature columns to start with 'f-' for easier tracking of features:  \n",
    "- f- -> features  \n",
    "- f-cat- -> categorical (no point in scaling)  \n",
    "- f-c- -> CELLS  \n",
    "- f-g- -> GENES  \n",
    "- f-pca-c/g -> pca based on c/g  \n",
    "- f-model_A- -> features created from model A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:45.664353Z",
     "start_time": "2020-11-18T19:14:45.526158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categoricals\n",
    "train = train.rename(columns={'cp_time': 'f-cat-cp_time', \n",
    "                              'cp_dose': 'f-cat-cp_dose'})\n",
    "test = test.rename(columns={'cp_time': 'f-cat-cp_time', \n",
    "                            'cp_dose': 'f-cat-cp_dose'})\n",
    "# Cells and Genes\n",
    "train.columns = ['f-' + col if (col.startswith('c-') or col.startswith('g-')) else col for col in train.columns]\n",
    "test.columns = ['f-' + col if (col.startswith('c-') or col.startswith('g-')) else col for col in test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:47.229072Z",
     "start_time": "2020-11-18T19:14:47.152234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT_COLS: 2\n",
      "GENES: 772\n",
      "CELLS: 100\n",
      "\n",
      "Temp Feature Columns: 874\n",
      "Target Columns: 206\n",
      "Nonscored target Columns: 402\n"
     ]
    }
   ],
   "source": [
    "# List of column names for easy reference throughout\n",
    "CAT_COLS = [col for col in test.columns if col.startswith('f-cat-')]\n",
    "GENES = [col for col in test.columns if col.startswith('f-g-')]\n",
    "CELLS = [col for col in test.columns if col.startswith('f-c-')]\n",
    "temp_feature_cols = [col for col in test.columns if col.startswith('f-')]\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "targets_nonscored_cols = targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "# feature_cols not defined here because new features will be created and some will be dropped\n",
    "\n",
    "# Column Data Dimensions\n",
    "print(f\"CAT_COLS: {len(CAT_COLS)}\")\n",
    "print(f\"GENES: {len(GENES)}\")\n",
    "print(f\"CELLS: {len(CELLS)}\")\n",
    "print()\n",
    "print(f\"Temp Feature Columns: {len(temp_feature_cols)}\")\n",
    "print(f\"Target Columns: {len(target_cols)}\")\n",
    "print(f\"Nonscored target Columns: {len(targets_nonscored_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PreProcessing'></a>\n",
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:49.781652Z",
     "start_time": "2020-11-18T19:14:49.774825Z"
    }
   },
   "outputs": [],
   "source": [
    "if Quantile_Transforming:\n",
    "#     all_data = pd.concat([pd.DataFrame(train[G_and_C]), pd.DataFrame(test[G_and_C])])\n",
    "    non_cat_cols = list(set(temp_feature_cols) - set(CAT_COLS))\n",
    "    all_data = pd.DataFrame(train[non_cat_cols])\n",
    "    all_data_nans = all_data.copy()\n",
    "#     all_data_nans[all_data == -10] = np.nan\n",
    "\n",
    "    qt = QuantileTransformer(n_quantiles=100, output_distribution='normal', random_state=42)\n",
    "    qt.fit(all_data_nans.values)\n",
    "#     all_data_trans = qt.transform(all_data)\n",
    "\n",
    "    train.loc[:, non_cat_cols] = qt.transform(train.loc[:, non_cat_cols])\n",
    "    test.loc[:, non_cat_cols] = qt.transform(test.loc[:, non_cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Initial_Filtering'></a>\n",
    "# Initial Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:52.802819Z",
     "start_time": "2020-11-18T19:14:52.184076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var drops: 10\n"
     ]
    }
   ],
   "source": [
    "var_drops = []\n",
    "if Variance_Threshold:\n",
    "    non_cat_cols = list(set(temp_feature_cols) - set(CAT_COLS))\n",
    "    selector = VarianceThreshold(VAR_THRESH)\n",
    "    all_data = pd.concat([pd.DataFrame(train[non_cat_cols]), pd.DataFrame(test[non_cat_cols])])\n",
    "       \n",
    "    selector.fit(all_data[non_cat_cols])\n",
    "    kept_columns = all_data.columns.values[selector.get_support(indices=True)]\n",
    "    dropped_columns = set(non_cat_cols) - set(kept_columns)\n",
    "    \n",
    "    var_drops = dropped_columns\n",
    "\n",
    "print(f\"Var drops: {len(var_drops)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features from Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:53.697710Z",
     "start_time": "2020-11-18T19:14:53.557273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var Dropped Features: 10\n",
      "{'f-g-15', 'f-g-104', 'f-g-550', 'f-g-331', 'f-g-435', 'f-g-536', 'f-g-307', 'f-g-611', 'f-g-481', 'f-g-219'}\n",
      "Total Dropped Features: 10\n"
     ]
    }
   ],
   "source": [
    "dropped_features = var_drops\n",
    "\n",
    "# Incorporate Forced keep and drop features\n",
    "dropped_features = set(dropped_features).union(set(DROPPED_FEATURES))\n",
    "dropped_features = set(dropped_features) - set(KEPT_FEATURES)\n",
    "\n",
    "\n",
    "print(f\"Var Dropped Features: {len(var_drops)}\")\n",
    "print(var_drops)\n",
    "print(f\"Total Dropped Features: {len(dropped_features)}\")\n",
    "\n",
    "train.drop(columns=dropped_features, inplace=True)\n",
    "test.drop(columns=dropped_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:55.250562Z",
     "start_time": "2020-11-18T19:14:55.178738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT_COLS: 2\n",
      "GENES: 762\n",
      "CELLS: 100\n",
      "\n",
      "Temp Feature Columns: 864\n",
      "Target Columns: 206\n",
      "Nonscored target Columns: 402\n"
     ]
    }
   ],
   "source": [
    "# List of column names for easy reference throughout\n",
    "CAT_COLS = [col for col in test.columns if col.startswith('f-cat-')]\n",
    "GENES = [col for col in test.columns if col.startswith('f-g-')]\n",
    "CELLS = [col for col in test.columns if col.startswith('f-c-')]\n",
    "temp_feature_cols = [col for col in test.columns if col.startswith('f-')]\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "targets_nonscored_cols = targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "# feature_cols not defined here because new features will be created and some will be dropped\n",
    "\n",
    "# Column Data Dimensions\n",
    "print(f\"CAT_COLS: {len(CAT_COLS)}\")\n",
    "print(f\"GENES: {len(GENES)}\")\n",
    "print(f\"CELLS: {len(CELLS)}\")\n",
    "print()\n",
    "print(f\"Temp Feature Columns: {len(temp_feature_cols)}\")\n",
    "print(f\"Target Columns: {len(target_cols)}\")\n",
    "print(f\"Nonscored target Columns: {len(targets_nonscored_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Feature_Engineering'></a>\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -10 Boolean Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:57.548642Z",
     "start_time": "2020-11-18T19:14:57.541660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Boolean columns for -10 cases\n",
    "\n",
    "# Find Columns with an Unusual amount of -10 values -> I set a threshold of just 5 for now\n",
    "if Include_neg_10:\n",
    "    non_cat_cols = list(set(temp_feature_cols) - set(CAT_COLS))\n",
    "    count_neg_10 = (train[non_cat_cols] == -10).astype(int).sum(axis=0)\n",
    "    neg_10_cols = count_neg_10[count_neg_10 >= 5].index\n",
    "    neg_10_bool_cols = ['f-cat-10-' + col for col in neg_10_cols] \n",
    "    len(neg_10_cols)\n",
    "\n",
    "    train[neg_10_bool_cols] = 0\n",
    "    for col, new_col in zip(neg_10_cols, neg_10_bool_cols):\n",
    "        train.loc[train[train[col] == -10].index, new_col] = 1\n",
    "\n",
    "    test[neg_10_bool_cols] = 0\n",
    "    for col, new_col in zip(neg_10_cols, neg_10_bool_cols):\n",
    "        test.loc[test[test[col] == -10].index, new_col] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:14:58.936257Z",
     "start_time": "2020-11-18T19:14:58.862140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT_COLS: 2\n",
      "GENES: 762\n",
      "CELLS: 100\n",
      "\n",
      "Temp Feature Columns: 864\n",
      "Target Columns: 206\n",
      "Nonscored target Columns: 402\n"
     ]
    }
   ],
   "source": [
    "# List of column names for easy reference throughout\n",
    "CAT_COLS = [col for col in test.columns if col.startswith('f-cat-')]\n",
    "GENES = [col for col in test.columns if col.startswith('f-g-')]\n",
    "CELLS = [col for col in test.columns if col.startswith('f-c-')]\n",
    "temp_feature_cols = [col for col in test.columns if col.startswith('f-')]\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "targets_nonscored_cols = targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "# feature_cols not defined here because new features will be created and some will be dropped\n",
    "\n",
    "# Column Data Dimensions\n",
    "print(f\"CAT_COLS: {len(CAT_COLS)}\")\n",
    "print(f\"GENES: {len(GENES)}\")\n",
    "print(f\"CELLS: {len(CELLS)}\")\n",
    "print()\n",
    "print(f\"Temp Feature Columns: {len(temp_feature_cols)}\")\n",
    "print(f\"Target Columns: {len(target_cols)}\")\n",
    "print(f\"Nonscored target Columns: {len(targets_nonscored_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Feature_Reduction'></a>\n",
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:01.648906Z",
     "start_time": "2020-11-18T19:15:01.051485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components selected: 72\n"
     ]
    }
   ],
   "source": [
    "# Only trying Cells right now\n",
    "\n",
    "if PCA_VAR:\n",
    "    data = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\n",
    "    scaler = MinMaxScaler()\n",
    "    data.loc[:, CELLS] = scaler.fit_transform(data[CELLS])\n",
    "    \n",
    "    pca_cells = PCA(n_components = PCA_VAR_THRESH)\n",
    "    data2 = pca_cells.fit_transform(data)\n",
    "    \n",
    "    num_pca_cell_comp = data2.shape[1]\n",
    "    print(f\"n_components selected: {num_pca_cell_comp}\")\n",
    "          \n",
    "    train2 = data2[:train.shape[0]]\n",
    "    test2 = data2[train.shape[0]:]\n",
    "    \n",
    "    train2 = pd.DataFrame(train2, columns=[f'f-pca-c-{i}' for i in range(num_pca_cell_comp)])\n",
    "    test2 = pd.DataFrame(test2, columns=[f'f-pca-c-{i}' for i in range(num_pca_cell_comp)])\n",
    "    \n",
    "#     train = train.drop(columns=CELLS)\n",
    "    train = pd.concat([train, train2], axis=1)\n",
    "#     test = test.drop(columns=CELLS)\n",
    "    test = pd.concat([test, test2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:04.081866Z",
     "start_time": "2020-11-18T19:15:04.007073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT_COLS: 2\n",
      "GENES: 762\n",
      "CELLS: 100\n",
      "\n",
      "Temp Feature Columns: 936\n",
      "Target Columns: 206\n",
      "Nonscored target Columns: 402\n"
     ]
    }
   ],
   "source": [
    "# List of column names for easy reference throughout\n",
    "CAT_COLS = [col for col in test.columns if col.startswith('f-cat-')]\n",
    "GENES = [col for col in test.columns if col.startswith('f-g-')]\n",
    "CELLS = [col for col in test.columns if col.startswith('f-c-')]\n",
    "temp_feature_cols = [col for col in test.columns if col.startswith('f-')]\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "targets_nonscored_cols = targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "# feature_cols not defined here because new features will be created and some will be dropped\n",
    "\n",
    "# Column Data Dimensions\n",
    "print(f\"CAT_COLS: {len(CAT_COLS)}\")\n",
    "print(f\"GENES: {len(GENES)}\")\n",
    "print(f\"CELLS: {len(CELLS)}\")\n",
    "print()\n",
    "print(f\"Temp Feature Columns: {len(temp_feature_cols)}\")\n",
    "print(f\"Target Columns: {len(target_cols)}\")\n",
    "print(f\"Nonscored target Columns: {len(targets_nonscored_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CV_Folds'></a>\n",
    "# CV Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:09.535409Z",
     "start_time": "2020-11-18T19:15:05.759198Z"
    }
   },
   "outputs": [],
   "source": [
    "# folds will be identical to train, but with a kfold column. \n",
    "# folds wll be used in the actual training\n",
    "folds = train.copy()\n",
    "mskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:09.542327Z",
     "start_time": "2020-11-18T19:15:09.536343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1143)\n",
      "(21948, 1144)\n",
      "(3624, 937)\n",
      "(21948, 207)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PyTorch_Setup'></a>\n",
    "# PyTorch Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:09.550723Z",
     "start_time": "2020-11-18T19:15:09.544322Z"
    }
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train/val/inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:09.563869Z",
     "start_time": "2020-11-18T19:15:09.552733Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    # Tell the model which mode it is in. At the very least this enables dropout for train mode only.\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    # Tell the model which mode it is in. At the very least this disables dropout for eval mode.\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if CLAMPING:\n",
    "            loss = loss_fn(torch.clamp(outputs, min=CLAMP_MIN, max=CLAMP_MAX), targets)\n",
    "            valid_preds.append(torch.clamp(outputs, min=CLAMP_MIN, max=CLAMP_MAX).detach().cpu().numpy())\n",
    "        else:\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_preds.append(outputs.detach().cpu().numpy())\n",
    "            \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    # Tell the model which mode it is in. At the very least this disables dropout for eval mode.\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "        if CLAMPING:\n",
    "            preds.append(torch.clamp(outputs, min=CLAMP_MIN, max=CLAMP_MAX).detach().cpu().numpy())\n",
    "        else:\n",
    "            preds.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Label Smoothing Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:10.865503Z",
     "start_time": "2020-11-18T19:15:10.858559Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        m = nn.Sigmoid()\n",
    "        \n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)\n",
    "#         loss = F.binary_cross_entropy_with_logits(inputs, targets,)\n",
    "        loss = F.binary_cross_entropy(inputs, targets,)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:12.353188Z",
     "start_time": "2020-11-18T19:15:12.341240Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size, verbose=False):\n",
    "        super(Model, self).__init__()    \n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    # https://www.kaggle.com/c/lish-moa/discussion/188651\n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-10)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-10\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # lesson learned here: do not use F.prelu. Worse results.\n",
    "        \n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        self.recalibrate_layer(self.fc1)\n",
    "        x = self.prelu1(self.fc1(x))\n",
    "\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.fc2)\n",
    "        x = self.prelu2(self.fc2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.fc3)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:28.616810Z",
     "start_time": "2020-11-18T19:15:28.608842Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(input_data):\n",
    "    data = input_data.copy()\n",
    "    non_cat_cols = list(set(temp_feature_cols) - set(CAT_COLS))\n",
    "    ###### Category Encoding-----------------------------------------------------\n",
    "    # One-Hot-Encoding\n",
    "    if Category_Encoding == \"OHE\":\n",
    "        data = pd.get_dummies(data, prefix='f-cat-OHE-', prefix_sep='', columns=CAT_COLS)\n",
    "        print(f\" Number of OHE columns added: {len([col for col in data.columns if col.startswith('f-cat-OHE-')])}\")\n",
    "    # Label-Encoding\n",
    "    \n",
    "    # Manual-Mapping\n",
    "    if Category_Encoding == \"Mapping\":\n",
    "        data.loc[:, 'f-cat-cp_time'] = data.loc[:, 'f-cat-cp_time'].map({24: 1, 48: 2, 72: 3}).values\n",
    "        data.loc[:, 'f-cat-cp_dose'] = data.loc[:, 'f-cat-cp_dose'].map({'D1': 0, 'D2': 1}).values\n",
    "    \n",
    "    ###### Normalizing------------------------------------------------------------\n",
    "    # normalizes the train against train and the test against test\n",
    "    if Normalization:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        data.loc[:, non_cat_cols] = scaler.fit_transform(data.loc[:, non_cat_cols])\n",
    "    \n",
    "    ###### Scaling\n",
    "    # scales the train against train and the test against test\n",
    "    if Scaling:\n",
    "        MinMaxscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        data.loc[:, non_cat_cols] = MinMaxscaler.fit_transform(data.loc[:, non_cat_cols])\n",
    "    \n",
    "    ###### Remove Skewness --------------------------------------------------------\n",
    "    # Removing Skewness\n",
    "    if Remove_Skewness:\n",
    "        for col in non_cat_cols:\n",
    "            if(abs(data[col].skew()) > 0.75):\n",
    "\n",
    "                if(data[col].skew() < 0): # neg-skewness\n",
    "                    data[col] = data[col].max() - data[col] + 1\n",
    "                    data[col] = np.sqrt(data[col])\n",
    "\n",
    "                else:\n",
    "                    data[col] = np.sqrt(data[col])\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training_Process'></a>\n",
    "## Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Fold Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:30.400314Z",
     "start_time": "2020-11-18T19:15:30.385277Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(X_train, y_train, X_test, proc_feature_columns, target_columns, HyperParams, fold, seed):\n",
    "    # List of global things passed into here that aren't identified\n",
    "    # - valid_loss_array = np.zeros((EPOCHS, NFOLDS))\n",
    "    # - seed_everything function to avoid randomness\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # trn_idx is never used, but val_idx is. Both kept because it looks cleaner.\n",
    "    trn_idx = X_train[X_train['kfold'] != fold].index\n",
    "    val_idx = X_train[X_train['kfold'] == fold].index\n",
    "    \n",
    "    # For the current fold, separate the train(80%) and val(20%) data by the kfold column\n",
    "    X_train_df = X_train[X_train['kfold'] != fold].reset_index(drop=True)\n",
    "    X_valid_df = X_train[X_train['kfold'] == fold].reset_index(drop=True)\n",
    "    y_train_df = y_train[X_train['kfold'] != fold].reset_index(drop=True)\n",
    "    y_valid_df = y_train[X_train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    # separate the data by column and put into respective arrays\n",
    "    x_train = X_train_df[proc_feature_columns].values\n",
    "    y_train = y_train_df[target_columns].values\n",
    "    x_valid = X_valid_df[proc_feature_columns].values\n",
    "    y_valid = y_valid_df[target_columns].values\n",
    "    \n",
    "    # init class with the given train and val data\n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    \n",
    "    # DataLoader is set up as a 'Map-style' dataset \n",
    "    # a.k.a. it utilizes the --getitem-- and --len-- protocols we defined in the class\n",
    "    \n",
    "    # shuffle is good for training because we do not want any significance to order while training. \n",
    "    # Once the weights are determined, shuffle doesn't matter(for val and test)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=HyperParams[\"BATCH_SIZE\"], shuffle=True, num_workers=5, pin_memory=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=HyperParams[\"BATCH_SIZE\"], shuffle=False, num_workers=5, pin_memory=True)\n",
    "    \n",
    "    # Our model is simply the layers and forward function. These parameters define the model\n",
    "    model = Model(\n",
    "        num_features=len(proc_feature_columns),\n",
    "        num_targets=len(target_columns),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Device is set to cuda if available, else cpu (in the hyperparameter section)\n",
    "    model.to(HyperParams[\"DEVICE\"])\n",
    "    \n",
    "    # model.parameters() returns an iterator over the module parameters (most commonly used such as in this case, for the optimizer)\n",
    "    # OneCycleLR looks to be a very fast learining rate convergence method.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=HyperParams[\"LEARNING_RATE\"], \n",
    "                                 weight_decay=HyperParams[\"WEIGHT_DECAY\"])\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n",
    "                                              pct_start=0.1, \n",
    "                                              div_factor=1e3, \n",
    "                                              max_lr=1e-2, \n",
    "                                              epochs=HyperParams[\"EPOCHS\"], \n",
    "                                              steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    if Label_Smoothing:\n",
    "        loss_tr = SmoothBCEwLogits(smoothing=SMOOTHING)\n",
    "    else:\n",
    "        loss_tr = nn.BCELoss()\n",
    "    \n",
    "    loss_val = nn.BCELoss()\n",
    "\n",
    "    # create array of zeros for oof predictions. Shape is num of observations x num targets\n",
    "    oof = np.zeros((X_train.shape[0], len(target_columns)))\n",
    "\n",
    "    # start off with a big number. Will update each cycle on improvements\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # currently set for 30\n",
    "    for epoch in range(HyperParams[\"EPOCHS\"]):\n",
    "        \n",
    "        # In train_fn: return the loss for the data through our model. \n",
    "            # No predictions. \n",
    "            # We step the optimizer and scheduler.\n",
    "        # In valid_fn: return loss and predictions\n",
    "            # No training is done, as in weights/parameters are left unchanged\n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, HyperParams[\"DEVICE\"])\n",
    "        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_val, validloader, HyperParams[\"DEVICE\"])\n",
    "        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        valid_loss_array[epoch, fold] = valid_loss\n",
    "        \n",
    "        # if valid loss is the best so far, save the corresponding model\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "            \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    # After training the model, load the best model based on validation results, and run once with the test data for its predictions\n",
    "    x_test = X_test[proc_feature_columns].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=HyperParams[\"BATCH_SIZE\"], shuffle=False)\n",
    "    \n",
    "    # not sure why the model is re-initialized here. Is, and hsould be, identical to the train/val model. \n",
    "    # The load_state is still necessary though.\n",
    "    model = Model(\n",
    "        num_features=len(proc_feature_columns),\n",
    "        num_targets=len(target_columns),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(HyperParams[\"DEVICE\"])\n",
    "    \n",
    "    predictions = np.zeros((len(X_test), len(target_columns)))\n",
    "    predictions = inference_fn(model, testloader, HyperParams[\"DEVICE\"])\n",
    "    \n",
    "    # oof: The oof prediction array\n",
    "    # predictions: The test set prediction array\n",
    "    return oof, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Fold Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:30.955286Z",
     "start_time": "2020-11-18T19:15:30.948350Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_k_fold(X_train, y_train, X_test, feature_columns, target_columns, HyperParams, seed):\n",
    "    # Set up the oof and predictions arrays to be filled each time.\n",
    "    oof = np.zeros((X_train.shape[0], len(target_columns)))\n",
    "    predictions = np.zeros((len(X_test), len(target_columns)))\n",
    "    \n",
    "    # Process Features here for X_train and X_test rather than every fold redoing it\n",
    "    X_train = process_data(X_train)\n",
    "    X_test = process_data(X_test)\n",
    "    proc_feature_columns = [c for c in X_train.columns if c.startswith('f-')]\n",
    "    \n",
    "    for fold in range(HyperParams[\"NFOLDS\"]):\n",
    "        oof_, pred_ = run_training(X_train, y_train, X_test, proc_feature_columns, target_columns, HyperParams, fold, seed)\n",
    "        \n",
    "        # predictions happen each fold, this method averages the predicitons over all of the folds.\n",
    "        predictions += pred_ / HyperParams[\"NFOLDS\"]\n",
    "        # Only a subset of the oof is nonzero each time corresponding to the fold\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Seed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:31.335709Z",
     "start_time": "2020-11-18T19:15:31.331694Z"
    }
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "HyperParams = {\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"NFOLDS\": NFOLDS,\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"HIDDEN_SIZE\": HIDDEN_SIZE,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input for Model A\n",
    "Model A will use the same parameters as our original model (for now). \n",
    "Model A trains with our original features but wiht the nonscored targets, to predict them. \n",
    "These predictions will then be used as features into our original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:15:33.475647Z",
     "start_time": "2020-11-18T19:15:33.189742Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of seeds to run over.\n",
    "SEEDS = [1, 2, 3, 4, 42]\n",
    "\n",
    "# list of target/feature column names - should be 206 targets of them\n",
    "target_columns = target_cols + targets_nonscored_cols\n",
    "feature_columns = [c for c in folds.columns if c.startswith('f-')]\n",
    "\n",
    "# training x and kfold column. Contains 'sig_id' column\n",
    "X_train = folds[['sig_id'] + feature_columns + ['kfold']]\n",
    "y_train = folds[['sig_id'] + target_cols].merge(targets_nonscored, on='sig_id', how='left')\n",
    "\n",
    "# test data. Includes 'sig_id' and features, no target columns or kfold\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This chunk is just putting a wrapper around the main training function 'run_k_fold'\n",
    "# To allow many SEEDs to be tested and averaged at once.\n",
    "\n",
    "oof = np.zeros((X_train.shape[0], len(target_columns)))\n",
    "predictions = np.zeros((len(X_test), len(target_columns)))\n",
    "valid_loss_array = np.zeros((HyperParams[\"EPOCHS\"], HyperParams[\"NFOLDS\"]))\n",
    "\n",
    "for seed in SEEDS:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(X_train.copy(), y_train.copy(), X_test.copy(), feature_columns, target_columns, HyperParams, seed)\n",
    "    oof += oof_ / len(SEEDS)\n",
    "    predictions += predictions_ / len(SEEDS)\n",
    "\n",
    "# Make new outputs later\n",
    "Model_A_validation_predictions = X_train[['sig_id']]\n",
    "Model_A_validation_predictions[target_columns] = oof\n",
    "\n",
    "Model_A_test_predictions = X_test[['sig_id']]\n",
    "Model_A_test_predictions[target_columns] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A Outputs\n",
    "Model A will output features to be used as input features for the train and test sets of Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Features with sig_id column\n",
    "Model_A_to_Model_B_train = Model_A_validation_predictions\n",
    "Model_A_to_Model_B_train.columns = ['f-model_A-' + col if (col != 'sig_id') else col for col in Model_A_to_Model_B_train.columns]\n",
    "# Test Features\n",
    "Model_A_to_Model_B_test = Model_A_test_predictions\n",
    "Model_A_to_Model_B_test.columns = ['f-model_A-' + col if (col != 'sig_id') else col for col in Model_A_to_Model_B_test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use columns with highest correlation (from unchanged target file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_targets_df = targets_nonscored.drop('sig_id', axis=1).loc[:, (targets_nonscored.drop('sig_id', axis=1).sum() > 0).values]\n",
    "corr_mtx = nonzero_targets_df.corr()\n",
    "corr_map = corr_mtx[corr_mtx>=.7]\n",
    "corr_map[corr_map == 1] = np.nan\n",
    "# Columns that contain high correlation\n",
    "corr_cols = corr_map.columns[(corr_map.sum(axis=1) >0).values]\n",
    "\n",
    "temp_corr_cols = ['f-model_A-' + col for col in corr_cols]\n",
    "\n",
    "print(f\"Number of Columns kept from Model A based on high Correlations {len(corr_cols)}\")\n",
    "\n",
    "Model_A_to_Model_B_train = Model_A_to_Model_B_train[['sig_id'] + temp_corr_cols]\n",
    "Model_A_to_Model_B_test = Model_A_to_Model_B_test[['sig_id'] + temp_corr_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Validation Loss by Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss for each fold over the epochs\n",
    "# valid_loss_array is an array of the validaiton loss for each fold for each epoch\n",
    "fold_names = [f\"Fold_{fold}\" for fold in range(NFOLDS)]\n",
    "val_loss_df = pd.DataFrame(data=valid_loss_array, columns=fold_names)\n",
    "p = sns.lineplot(data=val_loss_df, dashes=False)\n",
    "p.set(ylim=(0.003, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input for Model B\n",
    "Model B is trained to predict the <b>scored</b> targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of seeds to run over.\n",
    "# SEEDS = [42]\n",
    "\n",
    "# list of target/feature column names - should be 206 targets of them\n",
    "target_columns = target_cols\n",
    "feature_columns = [c for c in folds.columns if c.startswith('f-')]\n",
    "\n",
    "# training x and kfold column. Contains 'sig_id' column\n",
    "X_train = folds[['sig_id'] + feature_columns + ['kfold']]\n",
    "X_train = X_train.merge(Model_A_to_Model_B_train, on='sig_id', how='left')\n",
    "\n",
    "y_train = folds[['sig_id'] + target_columns]\n",
    "\n",
    "# test data. Includes 'sig_id' and features, no target columns or kfold\n",
    "X_test = test.copy()\n",
    "X_test = X_test.merge(Model_A_to_Model_B_test, on='sig_id', how='left')\n",
    "\n",
    "# redeclaring the feature columns now that I have modified the input sets for train/test. Not the cleanest.\n",
    "feature_columns = [c for c in X_train.columns if c.startswith('f-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This chunk is just putting a wrapper around the main training function 'run_k_fold'\n",
    "# To allow many SEEDs to be tested and averaged at once.\n",
    "\n",
    "oof = np.zeros((X_train.shape[0], len(target_columns)))\n",
    "predictions = np.zeros((len(X_test), len(target_columns)))\n",
    "valid_loss_array = np.zeros((HyperParams[\"EPOCHS\"], HyperParams[\"NFOLDS\"]))\n",
    "\n",
    "for seed in SEEDS:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(X_train.copy(), y_train.copy(), X_test.copy(), feature_columns, target_columns, HyperParams, seed)\n",
    "    oof += oof_ / len(SEEDS)\n",
    "    predictions += predictions_ / len(SEEDS)\n",
    "\n",
    "# Make new outputs later\n",
    "validation_predictions = X_train[['sig_id']]\n",
    "validation_predictions[target_columns] = oof\n",
    "test_predictions = X_test[['sig_id']]\n",
    "test_predictions[target_columns] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save OOF Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving OOF predictions on non-control set for analysis\n",
    "validation_predictions.to_csv('unprocessed_validation_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Validation Loss by Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss for each fold over the epochs\n",
    "# valid_loss_array is an array of the validaiton loss for each fold for each epoch\n",
    "fold_names = [f\"Fold_{fold}\" for fold in range(NFOLDS)]\n",
    "val_loss_df = pd.DataFrame(data=valid_loss_array, columns=fold_names)\n",
    "p = sns.lineplot(data=val_loss_df, dashes=False)\n",
    "p.set(ylim=(0.014, 0.020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_y_true = target[target_cols].values\n",
    "raw_y_pred = validation_predictions[target_cols].values\n",
    "\n",
    "raw_scores = []\n",
    "for i in range(len(target_cols)):\n",
    "    raw_scores.append(log_loss(raw_y_true[:, i], raw_y_pred[:, i]))\n",
    "    \n",
    "raw_scores_arr = np.asarray(raw_scores)\n",
    "print(\"Raw CV log_loss: \", raw_scores_arr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores_df = pd.DataFrame(raw_scores_arr, index=target_cols)\n",
    "raw_scores_df = raw_scores_df.sort_values(by=raw_scores_df.columns[0], ascending=True)\n",
    "raw_scores_df.columns = [\"Raw_Log_Loss\"]\n",
    "# raw_scores_df.tail(20).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(raw_y_true, raw_y_pred)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(raw_y_true.shape[1]):\n",
    "    roc_auc[i] = roc_auc_score(raw_y_true[:, i], raw_y_pred[:, i])\n",
    "    fpr[i], tpr[i], _ = roc_curve(raw_y_true[:, i], raw_y_pred[:, i])\n",
    "roc_arr_raw = np.asarray(list(roc_auc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(roc_arr_raw, bins = 30)\n",
    "plt.title(f'ROC Scores by Label: {np.round(roc_arr_raw.mean(), 4)} Average')\n",
    "\n",
    "raw_roc_mean = roc_arr_raw.mean()\n",
    "print(f\"Raw roc_score: {np.round(raw_roc_mean, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = np.argwhere(roc_arr_raw < 0.5)\n",
    "for col in bad_cols:\n",
    "    print(target_cols[col[0]])\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[col[0]], tpr[col[0]], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[col[0]])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Adjusted\n",
    "CV Loss reported above and in the plots above are only for non-control cases.   \n",
    "Below is the adjusted CV_loss for when control cases are hard-coded to 0 and included in loss statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_targets_scored is the original unaltered df.\n",
    "train_temp = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(validation_predictions, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "if FORCE_BAD_COLS:\n",
    "    for col in bad_cols:\n",
    "        valid_results.iloc[:, col[0]+1] = 1.75/3624\n",
    "        \n",
    "valid_results.loc[train_temp['cp_type'] == 'ctl_vehicle', 1:] = 0\n",
    "\n",
    "adj_y_true = train_targets_scored[target_cols].values\n",
    "adj_y_pred = valid_results[target_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV LogLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_scores = []\n",
    "for i in range(len(target_cols)):\n",
    "    adj_scores.append(log_loss(adj_y_true[:, i], adj_y_pred[:, i]))\n",
    "    \n",
    "adj_scores_arr = np.asarray(adj_scores)\n",
    "print(\"Adj CV log_loss: \", adj_scores_arr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_scores_df = pd.DataFrame(adj_scores_arr, index=target_cols)\n",
    "adj_scores_df = adj_scores_df.sort_values(by=adj_scores_df.columns[0], ascending=True)\n",
    "adj_scores_df.columns = [\"Adj_Log_Loss\"]\n",
    "\n",
    "log_loss_df = raw_scores_df.merge(adj_scores_df, left_index=True, right_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "log_loss_df.tail(20).plot.barh(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(adj_y_true, adj_y_pred)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(adj_y_true.shape[1]):\n",
    "    roc_auc[i] = roc_auc_score(adj_y_true[:, i], adj_y_pred[:, i])\n",
    "    fpr[i], tpr[i], _ = roc_curve(adj_y_true[:, i], adj_y_pred[:, i])\n",
    "roc_arr_adj = np.asarray(list(roc_auc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(roc_arr_adj, bins = 30)\n",
    "plt.title(f'ROC Scores by Label: {np.round(roc_arr_adj.mean(), 4)} Average')\n",
    "\n",
    "adj_roc_mean = roc_arr_adj.mean()\n",
    "print(f\"Adj roc_score: {np.round(adj_roc_mean, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using roc_arr_raw to show the improvement on earlier problem examples\n",
    "bad_cols = np.argwhere(roc_arr_raw < 0.5)\n",
    "for col in bad_cols:\n",
    "    print(target_cols[col[0]])\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[col[0]], tpr[col[0]], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[col[0]])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw CV log_loss: {raw_scores_arr.mean()}  \")\n",
    "print(f\"Adj CV log_loss: {adj_scores_arr.mean()}  \")\n",
    "print(f\"Raw roc_score: {np.round(raw_roc_mean, 4)}  \")\n",
    "print(f\"Adj roc_score: {np.round(adj_roc_mean, 4)}  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Prepare_Submission_File'></a>\n",
    "# Prepare Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T18:59:50.987955Z",
     "start_time": "2020-11-18T18:59:50.937031Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4bdc078d8505>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Here is the part where cp_type == ctl_vehicle is handled.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# We merge on sig_id, so the sig_id's missing from our test predictions(only the ctl-vehicle set) remain 0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/lish-moa/test_features.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sig_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Here is the part where cp_type == ctl_vehicle is handled. \n",
    "# We merge on sig_id, so the sig_id's missing from our test predictions(only the ctl-vehicle set) remain 0.\n",
    "test_temp = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test_predictions, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "if FORCE_BAD_COLS:\n",
    "    for col in bad_cols:\n",
    "        sub.iloc[:, col[0]] = 1.75/3624\n",
    "\n",
    "sub.loc[test_temp['cp_type'] == 'ctl_vehicle', 1:] = 0\n",
    "        \n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
